---
title: 'Lending Club Interest Rate Study in R.'
date: '8/3/2021'
output:
  html_document: 
    theme: readable
    toc: yes  
    code_folding: show
  pdf_document: default
css: report.css
urlcolor: cyan
editor_options: 
  chunk_output_type: console
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center",
        fig.height = 5, fig.width = 7)
```

*Utility functions defined here, click `Code` to see*

```{r message=FALSE, warning=FALSE, class.source = 'fold-hide'}
library(knitr)
library(kableExtra)
library(dplyr)
library(MASS)

format_numerics <- function(data, digits = 2, notation_threshold = 0.00001) {
  # make sure is a data.frame, then format
  if(!is.data.frame(data)){
    data <- as.data.frame(data)
  }  
  data %>% 
    mutate_if(
      is.numeric, 
      function(x) {
        if_else(
          abs(x) < notation_threshold, 
          formatC(x, digits = digits, format = "e"), 
          formatC(x, digits = digits, format = "f", drop0trailing = FALSE)
        )
      }
    )
}

gen_kable <- function(table_data, add_row_names = TRUE, caption = "", foot_text = "", col_names = c(), row_names = c()) {
  f_data <- format_numerics(table_data) 
  if(length(col_names) != 0){
    colnames(f_data) <- col_names
  }
  if(length(row_names) != 0){
    rownames(f_data) <- row_names
  }  
  f_data %>%
  kable(., format = "html", row.names = add_row_names,
        caption = caption, 
        escape = FALSE) %>%
    kable_styling(bootstrap_options = c("striped", "hover"),
                  full_width = F,
                  font_size = 14,
                  position = "center") %>%
    footnote(general = foot_text)
}

```

# Introduction

## About the Data

The dataset contains background data on loans made through the LendingClub platform, an online peer-to-peer lender in which investors were able provide personal loans to borrowers. The platform shut down in 2020. The data was publicly available through the LendingClub website prior to its shutdown, but is included in the `openintro` package in R.

*From the R documentation:*

> This data set represents thousands of loans made through the Lending Club platform, which is a platform that allows individuals to lend to other individuals. Of course, not all loans are created equal. Someone who is a essentially a sure bet to pay back a loan will have an easier time getting a loan with a low interest rate than someone who appears to be riskier. And for people who are very risky? They may not even get a loan offer, or they may not have accepted the loan offer due to a high interest rate. It is important to keep that last part in mind, since this data set only represents loans actually made, i.e. do not mistake this data for loan applications!

The dataset can be accessed by installing the `R package` below.

```{r, eval=FALSE}
install.packages("openintro")
```

```{r, message=FALSE}
library("openintro")
```

Number of variables: **`r dim(loans_full_schema)[2]`**

Number of observations: **`r dim(loans_full_schema)[1]`**

## Statement of interest

We are interested in this dataset because it is a good example of the kind of data that will be analyzed in the private sector. Many industries consider income and credit history in making business decisions. Not only will this provide a quality learning opportunity involving real-world data, but will also provide a tangible example of statistical modeling experience that we can use as we continue our data science careers.

## Goal of the Model

This study aims to to find a `model` that can predict the  `interest_rate` given the available variables excluding `grade` and `subgrade`. 

By fitting a simple model using the `grade` we can see a high $R^2$.

```{r}
model_with_grade = lm(interest_rate ~ grade, data = loans_full_schema)

r2_w_grade = summary(model_with_grade)$r.squared
```

Given the high $R^2$ value of `r r2_w_grade`, it seems like it explains our `response` variable very well. This is confirmed by the [LendingClub documentation](https://www.lendingclub.com/foliofn/rateDetail.action), which indicates that each grade/subgrade combo corresponds to a different interest rate.

Our goal in this study is to look for a model using a set of the other predictors that can also explain the `interest_rate`.

# Methods

## Utility functions.

Here we define `functions` that will be of use during the study.

The following `function` will help us to report diagnostics such as:

- Normality.
- Constant Variance.

```{r, message = FALSE, warning = FALSE}
library(lmtest)

diagnostics <- function(model, alpha = 0.05) {

  diags <- list()
  p_val_st <- shapiro.test(model$residuals)$p.value
  p_val_bp <- bptest(model)$p.value
  diags$shapiro <- list(
    "p_val" = p_val_st,
    "decision" = ifelse(p_val_st < alpha, "Reject", "Fail to Reject"))
  diags$bptest <- list(
    "p_val" = p_val_bp,
    "decision" = ifelse(p_val_bp < alpha, "Reject", "Fail to Reject"))
  
  diags
}
```

The function below will show the `QQ` and `Fitted vs Residuals` plots for a given model.

```{r}
diagnostic_graph <- function(model, pcol = "grey", lcol = "dodgerblue") {
  par(mfrow = c(1, 2), bg="ghostwhite")
  plot(model$fitted.values, model$residuals,  
       col = pcol, pch = 20,
       xlab = "Fitted", ylab = "Residuals",
       main = "Fitted vs Residuals")
  abline(h = 0, col = lcol, lwd = 2)
  qqnorm(model$residuals, main = "Normal Q-Q Plot", 
         col = pcol)
  qqline(model$residuals, col = lcol, lwd = 2)
}
```


## Variables Selection and Data Preparation

For simplicity we start removing a few columns that have many `NA` values. We will also remove some irrelevant variables like dates and descriptions. Additionally, we will remove `grade`, `subgrade`, and the results of the interest rate - `installment`, `balance`, and the details on payments.

```{r}
## Remove columns with many NAs
loans_data <- subset(loans_full_schema, 
                     select = -c(num_accounts_120d_past_due,
                                 months_since_last_credit_inquiry,
                                 months_since_90d_late,
                                 months_since_last_delinq,
                                 debt_to_income_joint,
                                 verification_income_joint,
                                 annual_income_joint,
                                 emp_length
                     ))

## Remove unimportant
loans_data <- subset(loans_data, 
                     select = -c(
                       state,
                       emp_title,
                       issue_month
                     ))

## Remove 24 entrie with NA dept_to_income
loans_data <- na.omit(loans_data)

## Remove some categoricals
loans_data <- subset(loans_data, 
                     select = -c(
                       loan_status,
                       initial_listing_status,
                       disbursement_method,
                       grade,
                       sub_grade
                     ))

## Conceptually we should remove the following variables,
## as they are a result of the interest rate.
loans_data <- subset(loans_data, 
                     select = -c(
                      paid_total,
                      paid_principal,
                      paid_interest,
                      paid_late_fees,
                      installment,
                      balance
                     ))
```

We will also convert some character-structured data to factors. Additionally, although `term` appears to be numerical, it appears there are only two options for loan terms - a 3-year loan (36 months) and a 5-year loan (60 months). Thus, we will structure this as a factor.

```{r}
loans_data$homeownership <- as.factor(loans_data$homeownership)
loans_data$verified_income <- as.factor(loans_data$verified_income)
loans_data$application_type <- as.factor(loans_data$application_type)
loans_data$term <- as.factor(loans_data$term)
```

## Collinearity Analysis (Bryan)

Now that we have divided our data, we will further analyze for collinear variables.

```{r}
##Numerical dataframe for correlations
loans_num = subset(loans_data, select = -c(homeownership, verified_income, loan_purpose, application_type, term))

round(cor(loans_num), 3)

cor(loans_num) > 0.8
```

The first thing we notice is that we get `NA` values for the correlations on `current_accounts_delinq` and `num_accounts_30d_past_due`. For simplicities sake, we will remove these predictors.

Additionally, we notice that the following pairs are highly collinear (correlation > 0.8):

`open_credit_lines` and `num_satisfactory_accounts`
`open_credit_lines` and `num_open_cc_accounts`
`num_open_cc_accounts` and `num_satisfactory_accounts`
`num_active_debit_accounts` and `num_cc_carrying_balance`
`num_total_cc_accounts` and `num_open_cc_accounts`
`num_historical_failed_to_pay` and `tax_liens`

From these pairs, we will remove `num_open_cc_accounts` and `open_credit_lines`, as each correlates highly with multiple variables. Next we will fit a basic additive model with the remaining predictors for the purpose of analyzing the VIF.

```{r}
loans_data = subset(loans_data, select = -c(current_accounts_delinq,
                                            num_accounts_30d_past_due,
                                            num_open_cc_accounts,
                                            open_credit_lines))

add_model_vif = lm(interest_rate ~ ., data = loans_data)
vif(add_model_vif)[vif(add_model_vif) > 5]
```

One of the first things I notice in this VIF is the abundance of `loan_purpose`. As loan purpose is a factor variable with 14 levels, it "clogs" the model with dummy variables. Conceptually, it also makes sense that it would be highly collinear - somebody obtaining a loan for "debt consolidation", for instance, likely has indicators of high debt in the other predictors. Thus, we will remove that variable and refit the model.

```{r}
loans_data = subset(loans_data, select = -c(loan_purpose))
add_model_vif = lm(interest_rate ~ ., data = loans_data)
vif(add_model_vif)[vif(add_model_vif) > 5]
```

It appears that `loan_amount` has by far the largest collinearity, so we will remove that from the dataset as well.

```{r}
loans_data = subset(loans_data, select = -c(loan_amount))
add_model_vif = lm(interest_rate ~ ., data = loans_data)
vif(add_model_vif)[vif(add_model_vif) > 5]
```

Now, all predictors have a VIF value below 10, but still above our target benchmark of 5. We will continue to remove the highest VIF from our predictors.

```{r}
loans_data = subset(loans_data, select = -c(num_satisfactory_accounts))
add_model_vif = lm(interest_rate ~ ., data = loans_data)
vif(add_model_vif)[vif(add_model_vif) > 5]
```

While the remaining VIF values are close to our benchmark of 5, we will choose to remove `total_credit_lines` for consistency sake. Removing this variable eliminates all collinearity issues.

```{r}
loans_data = subset(loans_data, select = -c(total_credit_lines))
add_model_vif = lm(interest_rate ~ ., data = loans_data)
vif(add_model_vif)
```

Divide Training and Testing Data.

```{r}
# Remove before submitting !!
set.seed(420)

idxs <- 1:nrow(loans_data)
trn_idxs <- sample(idxs, 5000) # CHANGE before submitting

##### Remove before submitting
#sample_idxs <- sample(idxs, 1500)
#loans_data <- loans_data[sample_idxs, ]
#####

# Train and Test sets
loans_train <- loans_data[trn_idxs, ]
loans_test <- loans_data[-trn_idxs, ]
```

## Transformations (Alejandro)

A first check into the `Linear Model Assuptions` suggests we might need to try some transformations.

```{r, fig.height = 5, fig.width = 7, fig.align = "center"}
additive_mod <- lm(interest_rate ~ ., data = loans_train)

diagnostic_graph(additive_mod)
```

### Response Transformation

Since our response variable `interest_rate` is a positive `numeric` value, we explore a `Box-Cox Transformation` as defined below.

$$
g_\lambda(y) = \left\{
\begin{array}{lr}\displaystyle\frac{y^\lambda - 1}{\lambda} &  \lambda \neq 0\\
        & \\
       \log(y) &  \lambda = 0
     \end{array}
   \right.
$$

```{r, fig.height = 5, fig.width = 7, fig.align = "center"}
par(mfrow=c(1, 1), bg="ghostwhite")
boxcox(additive_mod, plotit = TRUE, lambda = seq(-0.8, 0.8, by = 0.1))
```

Since $\lambda \approx 0$, we try to apply a $log$ `transformation` of the `response`.

```{r, fig.height = 5, fig.width = 7, fig.align = "center"}
additive_mod <- lm(log(interest_rate) ~ ., data = loans_train)

diagnostic_graph(additive_mod)
```

We noticed a significant improvement in the `QQ Plot`, yet we still see an obvious pattern in the `Fitted vs Residuals` plot.

### Predictors Transformation

Below plots show the relation between 4 the predictors and the response.

```{r, fig.height = 5, fig.width = 7, fig.align = "center"}
par(mfrow=c(2, 2), bg="ghostwhite")

plot(interest_rate ~ accounts_opened_24m, 
     xlab = "Predictor (accounts_opened_24m)",
     ylab = "Response (interest_rate)",
     data = loans_data, col = "deepskyblue3", pch = 20,
     main = "Accounts Opened vs Interest Rate")

plot(interest_rate ~ total_debit_limit, 
     xlab = "Predictor (total_debit_limit)",
     ylab = "Response (interest_rate)",
     data = loans_data, col = "deepskyblue3", pch = 20,
     main = "Total Debit Limit vs Interest Rate")

plot(interest_rate ~ num_total_cc_accounts, 
     xlab = "Predictor (num_total_cc_accounts)",
     ylab = "Response (interest_rate)",
     data = loans_data, col = "deepskyblue3", pch = 20,
     main = "Number of Credit Card Accounts vs Interest Rate")

plot(interest_rate ~ total_credit_limit, 
     xlab = "Predictor (total_credit_limit)",
     ylab = "Response (interest_rate)",
     data = loans_data, col = "deepskyblue3", pch = 20,
     main = "Total Credit Limit vs Interest Rate")

```

From the plots above, there is no obvious correlation from which we can decide a transformation to apply. In fact, we tried a few of them without noticing much improvement in the `QQ plot` and `Fitted vs Residuals`.

## Model Selection (Joshua)

```{r}
additive = lm(interest_rate ~ . - num_cc_carrying_balance - num_historical_failed_to_pay - num_total_cc_accounts, data = loans_train)

n = length(resid(additive))
selected_aic = step(additive, direction = "backward", trace = FALSE)
selected_bic = step(additive, direction = "backward", k = log(n), trace = FALSE)
```


```{r}
log_additive = lm(log(interest_rate) ~ .
- num_cc_carrying_balance - num_historical_failed_to_pay  - num_total_cc_accounts, data = loans_train)

n = length(resid(log_additive))
log_selected_aic = step(log_additive, direction = "backward", trace = FALSE)
log_selected_bic = step(log_additive, direction = "backward", k = log(n), trace = FALSE)
```

##Generate interactive model from best selected

```{r}
interactive = lm(interest_rate ~ (verified_income + debt_to_income + 
    delinq_2y + inquiries_last_12m + total_credit_limit + 
    total_credit_utilized + accounts_opened_24m + num_active_debit_accounts + 
    total_debit_limit + num_mort_accounts + account_never_delinq_percent + 
    term) ^ 2, data = loans_train)

log_interactive = lm(log(interest_rate) ~ (verified_income + debt_to_income + 
    delinq_2y + inquiries_last_12m + total_credit_limit + 
    total_credit_utilized + accounts_opened_24m + num_active_debit_accounts + 
    total_debit_limit + num_mort_accounts + account_never_delinq_percent + 
    public_record_bankrupt + term) ^ 2, data = loans_train)
```


## Unusual Observations (Bryan)

- leverage
- outliers
- influence

## Model diagnostics for assumptions (Alejandro)

Below we use our function to diagnose the models assumptions. We used the `diagnostics` function defined above. The function calculates both `shapiro` and `bptest`. We use $alpha = 0.05$

```{r}

sample_idxs <- sample(idxs, 5000)
loans_data_small <- loans_data[sample_idxs, ]

## Convert Joshua's code to functions so I can reuse them here with a
## smaller data set.

additive_diag = diagnostics(additive)
selected_aic_diag = diagnostics(selected_aic)
selected_bic_diag = diagnostics(selected_bic)

log_additive_diag = diagnostics(log_additive)
log_selected_aic_diag = diagnostics(log_selected_aic)
log_selected_bic_diag = diagnostics(log_selected_bic)

interactive_diag = diagnostics(interactive)
log_interactive_diag = diagnostics(log_interactive)

```


# Results

## Model Metrics

```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

adjrsquared = c(summary(additive)$adj.r.squared, summary(selected_aic)$adj.r.squared, summary(selected_bic)$adj.r.squared,
                summary(interactive)$adj.r.squared, summary(log_additive)$adj.r.squared, summary(log_selected_aic)$adj.r.squared,
                summary(log_selected_bic)$adj.r.squared, summary(log_interactive)$adj.r.squared)
loocv_rmse = c(calc_loocv_rmse(additive), calc_loocv_rmse(selected_aic), calc_loocv_rmse(selected_bic), calc_loocv_rmse(interactive),
               calc_loocv_rmse(log_additive), calc_loocv_rmse(log_selected_aic), calc_loocv_rmse(log_selected_bic), calc_loocv_rmse(log_interactive))

model_metrics <- data.frame (Model=c("Additive", "Backwards AIC", "Backwards BIC", "Interactive", "Additive(log)", "Backwards AIC(log)", "Backwards BIC(log)", "Interactive(log)"),
                  "Adjusted R^2" = adjrsquared,
                  "LOOCV-RMSE" = loocv_rmse,
                  check.names=FALSE)

gen_kable(model_metrics,
          caption = "Model Metrics",
          foot_text = "Table 1")
```

# Model diagnostics

```{r}
diags_data = data.frame(
  row.names = c("Additive", "Backwards AIC", "Backwards BIC", "Interactive", "Additive(log)", "Backwards AIC(log)", "Backwards BIC(log)", "Interactive(log)"),
  "Shapiro Test p-value" = c(additive_diag$shapiro$p_val,selected_aic_diag$shapiro$p_val,selected_bic_diag$shapiro$p_val,log_additive_diag$shapiro$p_val,log_selected_aic_diag$shapiro$p_val,log_selected_bic_diag$shapiro$p_val,interactive_diag$shapiro$p_val,log_interactive_diag$shapiro$p_val),
  "BP test p-value" = c(additive_diag$bptest$p_val,selected_aic_diag$bptest$p_val,selected_bic_diag$bptest$p_val,log_additive_diag$bptest$p_val,log_selected_aic_diag$bptest$p_val,log_selected_bic_diag$bptest$p_val,interactive_diag$bptest$p_val,log_interactive_diag$bptest$p_val)
)

gen_kable(diags_data,
          col_names = c("Shapiro Test p-value", "BP test p-value"),
          caption = "Models Diagnostics",
          foot_text = "Table 1")

```


# Discussion

