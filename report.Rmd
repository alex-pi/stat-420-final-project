---
title: 'Lending Club Interest Rate Study in R.'
date: '8/3/2021'
output:
  html_document: 
    theme: readable
    toc: yes  
    code_folding: show
  pdf_document: default
css: report.css
urlcolor: cyan
editor_options: 
  chunk_output_type: console
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center",
        fig.height = 5, fig.width = 7)
```

*Utility functions defined here, click `Code` to see*

```{r message=FALSE, warning=FALSE, class.source = 'fold-hide'}
library(knitr)
library(kableExtra)
library(dplyr)
library(MASS)

format_numerics <- function(data, digits = 2, notation_threshold = 0.00001) {
  # make sure is a data.frame, then format
  if(!is.data.frame(data)){
    data <- as.data.frame(data)
  }  
  data %>% 
    mutate_if(
      is.numeric, 
      function(x) {
        if_else(
          abs(x) < notation_threshold, 
          formatC(x, digits = digits, format = "e"), 
          formatC(x, digits = digits, format = "f", drop0trailing = FALSE)
        )
      }
    )
}

gen_kable <- function(table_data, add_row_names = TRUE, caption = "", foot_text = "", col_names = c(), row_names = c()) {
  f_data <- format_numerics(table_data) 
  if(length(col_names) != 0){
    colnames(f_data) <- col_names
  }
  if(length(row_names) != 0){
    rownames(f_data) <- row_names
  }  
  f_data %>%
  kable(., format = "html", row.names = add_row_names,
        caption = caption, 
        escape = FALSE) %>%
    kable_styling(bootstrap_options = c("striped", "hover"),
                  full_width = F,
                  font_size = 14,
                  position = "center") %>%
    footnote(general = foot_text)
}

```

# Introduction

## About the Data

The dataset contains background data on loans made through the LendingClub platform, an online peer-to-peer lender in which investors were able provide personal loans to borrowers. The platform shut down in 2020. The data was publicly available through the LendingClub website prior to its shutdown, but is included in the `openintro` package in R.

*From the R documentation:*

> This data set represents thousands of loans made through the Lending Club platform, which is a platform that allows individuals to lend to other individuals. Of course, not all loans are created equal. Someone who is a essentially a sure bet to pay back a loan will have an easier time getting a loan with a low interest rate than someone who appears to be riskier. And for people who are very risky? They may not even get a loan offer, or they may not have accepted the loan offer due to a high interest rate. It is important to keep that last part in mind, since this data set only represents loans actually made, i.e. do not mistake this data for loan applications!

The dataset can be accessed by installing the `R package` below.

```{r, eval=FALSE}
install.packages("openintro")
```

```{r, message=FALSE}
library("openintro")
```

Number of variables: **`r dim(loans_full_schema)[2]`**

Number of observations: **`r dim(loans_full_schema)[1]`**

## Statement of interest

We are interested in this dataset because it is a good example of the kind of data that will be analyzed in the private sector. Many industries consider income and credit history in making business decisions. Not only will this provide a quality learning opportunity involving real-world data, but will also provide a tangible example of statistical modeling experience that we can use as we continue our data science careers.

## Goal of the Model

This study aims to to find a `model` that can predict the  `interest_rate` given the available variables excluding `grade` and `subgrade`. 

By fitting a simple model using the `grade` we can see a high $R^2$.

```{r}
model_with_grade = lm(interest_rate ~ grade, data = loans_full_schema)

r2_w_grade = summary(model_with_grade)$r.squared
```

Given the high $R^2$ value of `r r2_w_grade`, it seems like it explains our `response` variable very well. This is confirmed by the [LendingClub documentation](https://www.lendingclub.com/foliofn/rateDetail.action), which indicates that each grade/subgrade combo corresponds to a different interest rate.

Our goal in this study is to look for a model using a set of the other predictors that can also explain the `interest_rate`.

# Methods

## Utility functions.

Here we define `functions` that will be of use during the study.

The following `function` will help us to report diagnostics such as:

- Normality.
- Constant Variance.

```{r, message = FALSE, warning = FALSE}
library(lmtest)

diagnostics <- function(model, alpha = 0.05) {

  diags <- list()
  p_val_st <- shapiro.test(model$residuals)$p.value
  p_val_bp <- bptest(model)$p.value
  diags$shapiro <- list(
    "p_val" = p_val_st,
    "decision" = ifelse(p_val_st < alpha, "Reject", "Fail to Reject"))
  diags$bptest <- list(
    "p_val" = p_val_bp,
    "decision" = ifelse(p_val_bp < alpha, "Reject", "Fail to Reject"))
  
  diags
}
```

The function below will show the `QQ` and `Fitted vs Residuals` plots for a given model.

```{r}
diagnostic_graph <- function(model, pcol = "grey", lcol = "dodgerblue") {
  par(mfrow = c(1, 2), bg="ghostwhite")
  plot(model$fitted.values, model$residuals,  
       col = pcol, pch = 20,
       xlab = "Fitted", ylab = "Residuals",
       main = "Fitted vs Residuals")
  abline(h = 0, col = lcol, lwd = 2)
  qqnorm(model$residuals, main = "Normal Q-Q Plot", 
         col = pcol)
  qqline(model$residuals, col = lcol, lwd = 2)
}
```


## Variables Selection and Data Preparation

For simplicity we start removing a few columns that have many `NA` values. We will also remove some irrelevant variables like dates and descriptions. Additionally, we will remove `grade`, `subgrade`, and the results of the interest rate - `installment`, `balance`, and the details on payments.

```{r}
## Remove columns with many NAs
loans_data <- subset(loans_full_schema, 
                     select = -c(num_accounts_120d_past_due,
                                 months_since_last_credit_inquiry,
                                 months_since_90d_late,
                                 months_since_last_delinq,
                                 debt_to_income_joint,
                                 verification_income_joint,
                                 annual_income_joint,
                                 emp_length
                     ))

## Remove unimportant
loans_data <- subset(loans_data, 
                     select = -c(
                       state,
                       emp_title,
                       issue_month
                     ))

## Remove 24 entrie with NA dept_to_income
loans_data <- na.omit(loans_data)

## Remove some categoricals
loans_data <- subset(loans_data, 
                     select = -c(
                       loan_status,
                       initial_listing_status,
                       disbursement_method,
                       grade,
                       sub_grade
                     ))

## Conceptually we should remove the following variables,
## as they are a result of the interest rate.
loans_data <- subset(loans_data, 
                     select = -c(
                      paid_total,
                      paid_principal,
                      paid_interest,
                      paid_late_fees,
                      installment,
                      balance
                     ))
```

We will also convert some character-structured data to factors. Additionally, although `term` appears to be numerical, it appears there are only two options for loan terms - a 3-year loan (36 months) and a 5-year loan (60 months). Thus, we will structure this as a factor.

```{r}
loans_data$homeownership <- as.factor(loans_data$homeownership)
loans_data$verified_income <- as.factor(loans_data$verified_income)
loans_data$application_type <- as.factor(loans_data$application_type)
loans_data$term <- as.factor(loans_data$term)
```

## Collinearity Analysis

Now that we have processed and cleaned our data, we will further analyze for collinear variables.

```{r}
##Numerical dataframe for correlations
loans_num = subset(loans_data, select = -c(homeownership, verified_income, loan_purpose, application_type, term))

round(cor(loans_num), 3)

cor(loans_num) > 0.8
```

The first thing we notice is that we get `NA` values for the correlations on `current_accounts_delinq` and `num_accounts_30d_past_due`. For simplicity's sake, we will remove these predictors.

Additionally, we notice that the following pairs are highly collinear (correlation > 0.8):

- `open_credit_lines` and `num_satisfactory_accounts`
- `open_credit_lines` and `num_open_cc_accounts`
- `num_open_cc_accounts` and `num_satisfactory_accounts`
- `num_active_debit_accounts` and `num_cc_carrying_balance`
- `num_total_cc_accounts` and `num_open_cc_accounts`
- `num_historical_failed_to_pay` and `tax_liens`

From these pairs, we will remove `num_open_cc_accounts` and `open_credit_lines`, as each correlates highly with multiple variables. Next we will fit a basic additive model with the remaining predictors for the purpose of analyzing the VIF.

```{r, warning=FALSE, message=FALSE}
library(faraway)
loans_data = subset(loans_data, select = -c(current_accounts_delinq,
                                            num_accounts_30d_past_due,
                                            num_open_cc_accounts,
                                            open_credit_lines))

add_model_vif = lm(interest_rate ~ ., data = loans_data)
vif(add_model_vif)[vif(add_model_vif) > 5]
```

One of the first things we notice in this VIF is the abundance of `loan_purpose`. As loan purpose is a factor variable with 14 levels, it "clogs" the model with dummy variables. Conceptually, it also makes sense that it would be highly collinear - somebody obtaining a loan for "debt consolidation", for instance, likely has indicators of high debt in the other predictors. Thus, we will remove that variable and refit the model.

```{r, warning=FALSE}
loans_data = subset(loans_data, select = -c(loan_purpose))
add_model_vif = lm(interest_rate ~ ., data = loans_data)
vif(add_model_vif)[vif(add_model_vif) > 5]
```

It appears that `loan_amount` has by far the largest collinearity, so we will remove that from the dataset as well.

```{r, warning=FALSE}
loans_data = subset(loans_data, select = -c(loan_amount))
add_model_vif = lm(interest_rate ~ ., data = loans_data)
vif(add_model_vif)[vif(add_model_vif) > 5]
```

Now, all predictors have a VIF value below 10, but still above our target benchmark of 5. We will continue to remove the highest VIF from our predictors.

```{r, warning=FALSE}
loans_data = subset(loans_data, select = -c(num_satisfactory_accounts))
add_model_vif = lm(interest_rate ~ ., data = loans_data)
vif(add_model_vif)[vif(add_model_vif) > 5]
```

While the remaining VIF values are close to our benchmark of 5, we will choose to remove `total_credit_lines` for consistency sake. Removing this variable eliminates all remaining collinearity issues.

```{r, warning=FALSE}
loans_data = subset(loans_data, select = -c(total_credit_lines))
add_model_vif = lm(interest_rate ~ ., data = loans_data)
vif(add_model_vif)
```

Divide Training and Testing Data.

```{r}
idxs <- 1:nrow(loans_data)
trn_idxs <- sample(idxs, 7000) 

# Train and Test sets
loans_train <- loans_data[trn_idxs, ]
loans_test <- loans_data[-trn_idxs, ]
```

## Transformations

A first check into the linear model assumptions suggests we might need to try some transformations.

```{r, fig.height = 5, fig.width = 7, fig.align = "center"}
additive_mod <- lm(interest_rate ~ ., data = loans_train)

diagnostic_graph(additive_mod)
```

### Response Transformation

Since our response variable `interest_rate` is a positive `numeric` value, we explore a `Box-Cox Transformation` as defined below.

$$
g_\lambda(y) = \left\{
\begin{array}{lr}\displaystyle\frac{y^\lambda - 1}{\lambda} &  \lambda \neq 0\\
        & \\
       \log(y) &  \lambda = 0
     \end{array}
   \right.
$$

```{r, fig.height = 5, fig.width = 7, fig.align = "center"}
par(mfrow=c(1, 1), bg="ghostwhite")
boxcox(additive_mod, plotit = TRUE, lambda = seq(-0.8, 0.8, by = 0.1))
```

Since $\lambda \approx 0$, we try to apply a log-transformation of the response.

```{r, fig.height = 5, fig.width = 7, fig.align = "center"}
additive_mod <- lm(log(interest_rate) ~ ., data = loans_train)

diagnostic_graph(additive_mod)
```

We noticed a significant improvement in the `QQ Plot`, yet we still see an obvious pattern in the `Fitted vs Residuals` plot.

### Predictors Transformation

The below plots show the relation between four of the predictors and the response.

```{r, fig.height = 5, fig.width = 7, fig.align = "center"}
par(mfrow=c(2, 2), bg="ghostwhite")

plot(interest_rate ~ accounts_opened_24m, 
     xlab = "Predictor (accounts_opened_24m)",
     ylab = "Response (interest_rate)",
     data = loans_data, col = "deepskyblue3", pch = 20,
     main = "Accounts Opened vs Interest Rate")

plot(interest_rate ~ total_debit_limit, 
     xlab = "Predictor (total_debit_limit)",
     ylab = "Response (interest_rate)",
     data = loans_data, col = "deepskyblue3", pch = 20,
     main = "Total Debit Limit vs Interest Rate")

plot(interest_rate ~ num_total_cc_accounts, 
     xlab = "Predictor (num_total_cc_accounts)",
     ylab = "Response (interest_rate)",
     data = loans_data, col = "deepskyblue3", pch = 20,
     main = "Number of Credit Card Accounts vs Interest Rate")

plot(interest_rate ~ total_credit_limit, 
     xlab = "Predictor (total_credit_limit)",
     ylab = "Response (interest_rate)",
     data = loans_data, col = "deepskyblue3", pch = 20,
     main = "Total Credit Limit vs Interest Rate")

```

From the plots above, there is no obvious correlation from which we can decide a transformation to apply. In fact, we tried a few of them without noticing much improvement in the `QQ plot` and `Fitted vs Residuals`.

## Model Selection

```{r}
generate_models = function(training_data, subset_model = TRUE) {
  additive = lm(interest_rate ~ ., data = training_data, subset=subset_model)
  
  n = length(resid(additive))
  selected_aic = step(additive, direction = "backward", trace = FALSE)
  selected_bic = step(additive, direction = "backward", k = log(n), trace = FALSE)
  
  #generate an interaction model from selected aic
  interaction_formula = update(as.formula(selected_aic), ~. ^2)
  interactive = lm(formula=interaction_formula, data = training_data, subset=subset_model)
  
  log_additive = lm(log(interest_rate) ~ ., data = training_data, subset=subset_model)
  
  n = length(resid(log_additive))
  log_selected_aic = step(log_additive, direction = "backward", trace = FALSE)
  log_selected_bic = step(log_additive, direction = "backward", k = log(n), trace = FALSE)
  
  #generate an interaction model from log selected aic
  interaction_formula = update(as.formula(log_selected_aic), ~. ^2)
  log_interactive = lm(formula=interaction_formula, data = training_data, subset=subset_model)
  
  newList = list("additive" = additive, "selected_aic" = selected_aic, "selected_bic" = selected_bic, "interactive" = interactive, 
                 "log_additive" = log_additive, "log_selected_aic" = log_selected_aic, "log_selected_bic" = log_selected_bic, "log_interactive" = log_interactive)
  return (newList)
}

models = generate_models(loans_train)

```

## Unusual Observations

### Leverage

Below, we will investigate our fit models for high-leverage data points, using the heuristic that defines a high-leverage observation as one with a leverage of two times the average.

```{r}
#Change to use fit models

sum(hatvalues(models$additive) > 2 * mean(hatvalues(models$additive)))

sum(hatvalues(models$selected_aic) > 2 * mean(hatvalues(models$selected_aic)))

sum(hatvalues(models$selected_bic) > 2 * mean(hatvalues(models$selected_bic)))

sum(hatvalues(models$log_additive) > 2 * mean(hatvalues(models$log_additive)))

sum(hatvalues(models$log_selected_aic) > 2 * mean(hatvalues(models$log_selected_aic)))

sum(hatvalues(models$log_selected_bic) > 2 * mean(hatvalues(models$log_selected_bic)))

sum(hatvalues(models$interactive) > 2 * mean(hatvalues(models$interactive)))

sum(hatvalues(models$log_interactive) > 2 * mean(hatvalues(models$log_interactive)))

```

We can see that the base additive models (`additive` and `log_additive`) have the fewest high-leverage observations with 278. The interaction model has the most high-leverage observations with 527.

### Outliers

We will run a similar procedure to investigate for outliers within each model. We will define an "outlier" as an observation that yields a standardized residual with a magnitude greater than 2. 

```{r}
sum(abs(rstandard(models$additive)) > 2)

sum(abs(rstandard(models$selected_aic)) > 2)

sum(abs(rstandard(models$selected_bic)) > 2)

sum(abs(rstandard(models$log_additive)) > 2)

sum(abs(rstandard(models$log_selected_aic)) > 2)

sum(abs(rstandard(models$log_selected_bic)) > 2)

sum(abs(rstandard(models$interactive)) > 2)

sum(abs(rstandard(models$log_interactive)) > 2)

```

Our log additive model has the fewest outliers, with 198. Close behind it is the AIC-selected log-transform model with 199. Our interaction model once again has the most outliers, with 273.

## Influence

Using the above results, it is likely that we have some influential observations. To determine this, we will calculate Cook's Distance in search of observations with a Cook's Distance greater than 4 / $n$, where $n$ is the number of observations.

```{r}
sum(cooks.distance(models$additive) > 4 / length(resid(models$additive)))

sum(cooks.distance(models$selected_aic) > 4 / length(resid(models$selected_aic)))

sum(cooks.distance(models$selected_bic) > 4 / length(resid(models$selected_bic)))

sum(cooks.distance(models$log_additive) > 4 / length(resid(models$log_additive)))

sum(cooks.distance(models$log_selected_aic) > 4 / length(resid(models$log_selected_aic)))

sum(cooks.distance(models$log_selected_bic) > 4 / length(resid(models$log_selected_bic)))

sum(cooks.distance(models$interactive) > 4 / length(resid(models$interactive)))

sum(cooks.distance(models$log_interactive) > 4 / length(resid(models$log_interactive)))

```

Once again, we find that the `log_additive` model contains the fewest influential observations with 188. The interaction model once again contains the most influential observations with 302.

Now, we will refit and compare coefficients from each model.

```{r}
add_refit = lm((models$additive)$call$formula, 
               data = loans_train,
               subset = cooks.distance(models$additive) <= 4 / length(resid(models$additive)))

summary(models$additive)$adj.r.squared
summary(add_refit)$adj.r.squared

knitr::kable(data.frame("Predictor" = names(coef(add_refit)),
           "full data" = unname(coef(models$additive)),
           "refit" = unname(coef(add_refit))), "pipe", align = "ccc")
```

From the above table, one can see that the largest changes in coefficients were all less than 1.0, and in many cases were around 0.1. We can also see that removing the influential observations improved the adjusted $R^2$ value by approximately 0.07.

```{r}
#Placeholder for other models - will copy/paste code from above
```


## Model diagnostics for assumptions

Below we use our function to diagnose the models assumptions. We used the `diagnostics` function defined above. The function calculates both `shapiro` and `bptest`. We use $alpha = 0.05$

```{r}

sample_idxs <- sample(idxs, 5000)
loans_data_small <- loans_data[sample_idxs, ]

models_diags = generate_models(loans_data_small)

additive_diag = diagnostics(models_diags$additive)
selected_aic_diag = diagnostics(models_diags$selected_aic)
selected_bic_diag = diagnostics(models_diags$selected_bic)

log_additive_diag = diagnostics(models_diags$log_additive)
log_selected_aic_diag = diagnostics(models_diags$log_selected_aic)
log_selected_bic_diag = diagnostics(models_diags$log_selected_bic)

interactive_diag = diagnostics(models_diags$interactive)
log_interactive_diag = diagnostics(models_diags$log_interactive)

```


# Results

## Model Metrics

```{r}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

adjrsquared = c(summary(models$additive)$adj.r.squared, summary(models$selected_aic)$adj.r.squared, summary(models$selected_bic)$adj.r.squared,
                summary(models$interactive)$adj.r.squared, summary(models$log_additive)$adj.r.squared, summary(models$log_selected_aic)$adj.r.squared,
                summary(models$log_selected_bic)$adj.r.squared, summary(models$log_interactive)$adj.r.squared)
loocv_rmse = c(calc_loocv_rmse(models$additive), calc_loocv_rmse(models$selected_aic), calc_loocv_rmse(models$selected_bic), calc_loocv_rmse(models$interactive),
               calc_loocv_rmse(models$log_additive), calc_loocv_rmse(models$log_selected_aic), calc_loocv_rmse(models$log_selected_bic), calc_loocv_rmse(models$log_interactive))

model_metrics <- data.frame (Model=c("Additive", "Backwards AIC", "Backwards BIC", "Interactive", "Additive(log)", "Backwards AIC(log)", "Backwards BIC(log)", "Interactive(log)"),
                  "Adjusted R^2" = adjrsquared,
                  "LOOCV-RMSE" = loocv_rmse,
                  check.names=FALSE)

gen_kable(model_metrics,
          caption = "Model Metrics",
          foot_text = "Table 1")
```

## Model Assumptions Results

`Table 2` below shows the `p-values` for the `Normality` and `Constant Variance` tests given the models we tried in this study.

```{r}
diags_data = data.frame(
  row.names = c("Additive", "Backwards AIC", "Backwards BIC", "Interactive", "Additive(log)", "Backwards AIC(log)", "Backwards BIC(log)", "Interactive(log)"),
  "Shapiro Test p-value" = c(additive_diag$shapiro$p_val,selected_aic_diag$shapiro$p_val,selected_bic_diag$shapiro$p_val,log_additive_diag$shapiro$p_val,log_selected_aic_diag$shapiro$p_val,log_selected_bic_diag$shapiro$p_val,interactive_diag$shapiro$p_val,log_interactive_diag$shapiro$p_val),
  "BP test p-value" = c(additive_diag$bptest$p_val,selected_aic_diag$bptest$p_val,selected_bic_diag$bptest$p_val,log_additive_diag$bptest$p_val,log_selected_aic_diag$bptest$p_val,log_selected_bic_diag$bptest$p_val,interactive_diag$bptest$p_val,log_interactive_diag$bptest$p_val)
)

gen_kable(diags_data,
          col_names = c("Shapiro Test p-value", "BP test p-value"),
          caption = "Models Diagnostics",
          foot_text = "Table 2")

```


Below we show the `QQ` and `Fitted vs Residuales` plots for 2 of those models. We take 1 additive and 1 interaction model for illustration.

**Plot 2.1 Additive model without response transformation**

```{r, fig.height = 5, fig.width = 7, fig.align = "center"}

diagnostic_graph(models$additive)
```

**Plot 2.2 Interactions model with $log$ transformation of the response**

```{r, fig.height = 5, fig.width = 7, fig.align = "center"}

diagnostic_graph(models$log_interactive)
```

## Prediction

```{r}
rmse = function(model, newdata) {
  pred = predict(model, new_data = newdata)
  
  return(sqrt(mean((newdata$interest_rate - pred) ^ 2)))
}

rmse_df = data.frame("Model" = c("Additive", "Add AIC", "Add BIC", "Log", "Log AIC", "Log BIC", "Interaction", "Log Interaction"),
                     "Train RMSE" = c(rmse(bryan_models$additive, loans_train),
                                      rmse(bryan_models$selected_aic, loans_train),
                                      rmse(bryan_models$selected_bic, loans_train),
                                      rmse(bryan_models$log_additive, loans_train),
                                      rmse(bryan_models$log_selected_aic, loans_train),
                                      rmse(bryan_models$log_selected_bic, loans_train),
                                      rmse(bryan_models$interactive, loans_train),
                                      rmse(bryan_models$log_interactive, loans_train)),
                     "Test RMSE" = c(rmse(bryan_models$additive, loans_test),
                                      rmse(bryan_models$selected_aic, loans_test),
                                      rmse(bryan_models$selected_bic, loans_test),
                                      rmse(bryan_models$log_additive, loans_test),
                                      rmse(bryan_models$log_selected_aic, loans_test),
                                      rmse(bryan_models$log_selected_bic, loans_test),
                                      rmse(bryan_models$interactive, loans_test),
                                      rmse(bryan_models$log_interactive, loans_test)))

knitr::kable(rmse_df, "pipe", align = "ccc")

```


# Discussion

Two sets of models were developed `interest_rate` as the response, however one set of models used the $log$ transformation of the response. We chose to build an additive model, which used all predictors which did not have collinearity issues. We also built a backwards AIC and BIC model from the additive models. Lastly we built an interaction model, however instead of using all predictors from the additive model, we chose to use the predictors from the AIC model to develop the two-way interaction model. In `Table 1` we observe both interactive models have the highest adjusted $r^2$ and the lowest LOOCV `RMSE`.

## Model Assumptions

`Table 2, Models Diagnostics` shows **extremely low** `p-values` so we can easily say that `Normality` and `Constant Variance` assumptions. Perhaps we missed some transformations of the `predictors` that could help getting better `p-values`. It is likely that linearity `Assumtion` is also violated.

One thing we clearly observed is that the `Box-Cox Transformation` (i.e. log), improves the `QQ Plot` considerably.

If we analyze the `p-values` for the $\hat\beta$ parameters in one of the fitted models we `could` argue that they are significant. The same could be said about the `Significance of the Regression` test.

```{r}
summ_example = summary(models$log_selected_bic)

gen_kable(summ_example$coef)
```

Unfortunately, we can hardly trust these results given the evidence of Assumption Violations found.



